{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N_PFMKG9lbtR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h5IpeUSMlqf6"
      },
      "outputs": [],
      "source": [
        "#Target model parameters\n",
        "target_model_batch_size_train = 64\n",
        "target_model_batch_size_test = 1000\n",
        "target_model_learning_rate = 0.001\n",
        "target_model_epochs = 3\n",
        "target_model_log_interval = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZNm1cF0Jl57I"
      },
      "outputs": [],
      "source": [
        "#Load digit data\n",
        "train_set = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "indices_01 = (train_set.targets == 0) | (train_set.targets == 1)\n",
        "train_set.data, train_set.targets = train_set.data[indices_01], train_set.targets[indices_01]\n",
        "\n",
        "train_set2 = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "indices_n01 = (train_set2.targets != 0) & (train_set2.targets != 1)\n",
        "train_set2.data, train_set2.targets = train_set2.data[indices_n01], train_set2.targets[indices_n01]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=target_model_batch_size_train, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "indices_n01 = (test_set.targets == 0) | (test_set.targets == 1)\n",
        "test_set.data, test_set.targets = test_set.data[indices_n01], test_set.targets[indices_n01]\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=target_model_batch_size_test, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T_j_yIyeoDh4"
      },
      "outputs": [],
      "source": [
        "#Create synthetic data set probs\n",
        "synth_probs = torch.zeros(784)\n",
        "for t,l in train_set:\n",
        "  synth_probs += torch.flatten(t,0,2)\n",
        "synth_probs /= len(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "40hWC6LpmWvB"
      },
      "outputs": [],
      "source": [
        "#Target model definition\n",
        "class TargetCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TargetCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SdSCimUAgkvC"
      },
      "outputs": [],
      "source": [
        "#Create the target model and optimizer\n",
        "target_model = TargetCNN()\n",
        "target_model_optimizer = optim.Adam(target_model.parameters(), lr=target_model_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uqOztwPklEh"
      },
      "outputs": [],
      "source": [
        "#Load target model if it exists\n",
        "if os.path.exists(\"target_model.pt\"):\n",
        "    checkpoint = torch.load(\"target_model.pt\")\n",
        "    target_model.load_state_dict(checkpoint['state_dict'])\n",
        "    target_model_optimizer.load_state_dict(checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thEXiIaRmpR-",
        "outputId": "b4b8a1f5-46f7-4ff6-c49d-d46e9094f289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 0% with avg loss 0.012038\n",
            "Epoch 1/3 51% with avg loss 0.001221\n",
            "Epoch 2/3 0% with avg loss 0.000697\n",
            "Epoch 2/3 51% with avg loss 0.000486\n",
            "Epoch 3/3 0% with avg loss 0.000389\n",
            "Epoch 3/3 51% with avg loss 0.000322\n",
            "Finished training with avg loss 0.000281\n"
          ]
        }
      ],
      "source": [
        "#Train the target model\n",
        "target_model.train()\n",
        "total_loss = 0\n",
        "num_examples = 0\n",
        "for e in range(target_model_epochs):\n",
        "  for b, (data, target) in enumerate(train_loader):\n",
        "    target_model_optimizer.zero_grad()\n",
        "    output = target_model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    total_loss += loss.item()\n",
        "    num_examples += len(data)\n",
        "    loss.backward()\n",
        "    target_model_optimizer.step()\n",
        "    if b % target_model_log_interval == 0:\n",
        "      print(\"Epoch {}/{} {:.0f}% with avg loss {:.6f}\".format(e+1,target_model_epochs,b/len(train_loader)*100,total_loss/num_examples))\n",
        "      checkpoint = {   \n",
        "                'state_dict': target_model.state_dict(),\n",
        "                'optimizer': target_model_optimizer.state_dict(),\n",
        "      }\n",
        "      torch.save(checkpoint, \"target_model.pt\")\n",
        "\n",
        "print(\"Finished training with avg loss {:.6f}\".format(total_loss/num_examples))\n",
        "checkpoint = {   \n",
        "          'state_dict': target_model.state_dict(),\n",
        "          'optimizer': target_model_optimizer.state_dict(),\n",
        "}\n",
        "torch.save(checkpoint, \"target_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G82HYgDqk6V5",
        "outputId": "ff11f3f6-311d-4827-ddea-7e4e5bbc8311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0011639842523556933\n"
          ]
        }
      ],
      "source": [
        "#Test the target model\n",
        "target_model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    output = target_model(data)\n",
        "    test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7rrrK8UdPo-w"
      },
      "outputs": [],
      "source": [
        "#Adaptive training parameters\n",
        "adaptive_retraining_model_learning_rate = 0.01\n",
        "adaptive_retraining_num_rounds = 64\n",
        "adaptive_retraining_queries_per_round = 16\n",
        "adaptive_retraining_uncertainty_threshold = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fb6aEBCuNPNP"
      },
      "outputs": [],
      "source": [
        "#Adaptive retraining model defintion\n",
        "class AdaptiveRetrainingModel(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(AdaptiveRetrainingModel, self).__init__()\n",
        "      self.fc1 = nn.Linear(784,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x.flatten(1,3)\n",
        "      x = F.log_softmax(self.fc1(x))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A4whjmnpPTzM"
      },
      "outputs": [],
      "source": [
        "#Create the adaptive retraining model\n",
        "adaptive_retraining_model = AdaptiveRetrainingModel()\n",
        "adaptive_retraining_model_optimizer = optim.Adam(adaptive_retraining_model.parameters(), lr=adaptive_retraining_model_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p5_aQS8PPVyw"
      },
      "outputs": [],
      "source": [
        "#Adaptive retraining query genereration defintion\n",
        "def AdaptiveRetraining():\n",
        "  adaptive_retraining_model.train()\n",
        "\n",
        "  #Store sequence of queries\n",
        "  queries = []\n",
        "\n",
        "  #Train the copy model on uniform random points\n",
        "  first_query_indecies = random.sample(range(0, len(train_set)-1), adaptive_retraining_queries_per_round)\n",
        "  for q_idx in first_query_indecies:\n",
        "    #Get query\n",
        "    q, a = train_set[q_idx]\n",
        "    q = q.unsqueeze(0)\n",
        "    #Get output of target model\n",
        "    r = target_model(q)\n",
        "    #Train the adaptive model\n",
        "    v = adaptive_retraining_model(q)\n",
        "    loss = F.cross_entropy(v, r)\n",
        "    loss.backward()\n",
        "    target_model_optimizer.step()\n",
        "    #Record the query\n",
        "    q = torch.flatten(q.squeeze(0),-2,-1)\n",
        "    queries.append(q)\n",
        "\n",
        "  #Train the copy model on new points along the decision boundary for several rounds\n",
        "  for r_num in range(adaptive_retraining_num_rounds-1):\n",
        "    for q_num in range(adaptive_retraining_queries_per_round):\n",
        "\n",
        "      #Find a query that the adaptive retraining model is uncertain about\n",
        "      q_idx = random.randint(0, len(train_set)-1)\n",
        "      num_attempts = 0\n",
        "      while(True):\n",
        "        #Get query\n",
        "        q, a = train_set[q_idx]\n",
        "        q = q.unsqueeze(0)\n",
        "        num_attempts += 1\n",
        "        #Get output of adaptive model\n",
        "        v = adaptive_retraining_model(q)\n",
        "        #Compute certainty heuristic\n",
        "        c = torch.max(v)-torch.mean(v)\n",
        "        #Keep query if unceratin or keep looking\n",
        "        if(c < adaptive_retraining_uncertainty_threshold):\n",
        "          break\n",
        "        elif(num_attempts >= 50):\n",
        "          break\n",
        "        else:\n",
        "          q_idx = random.randint(0, len(train_set)-1)\n",
        "\n",
        "      #Get output of target model\n",
        "      q, a = train_set[q_idx]\n",
        "      q = q.unsqueeze(1)\n",
        "      r = target_model(q)\n",
        "      #Train the adaptive model\n",
        "      v = adaptive_retraining_model(q)\n",
        "      loss = F.cross_entropy(v, r)\n",
        "      loss.backward()\n",
        "      target_model_optimizer.step()\n",
        "      #Record the query\n",
        "      q = torch.flatten(q.squeeze(0),-2,-1)\n",
        "      queries.append(q)\n",
        "      \n",
        "  \n",
        "  #Record the sequence of queries\n",
        "  queries = torch.stack(queries)\n",
        "  return torch.transpose(queries,0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X4uWRnr0iAyF"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQlc51tSbPe-"
      },
      "outputs": [],
      "source": [
        "#Benign queries parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y08FwoLRNaBF"
      },
      "outputs": [],
      "source": [
        "#Benign queries definition\n",
        "def BenignQueries(num_benign_queries):\n",
        "  num_pd = num_benign_queries//2\n",
        "  queries = []\n",
        "  query_indecies = random.sample(range(0, len(train_set)), num_benign_queries-num_pd)\n",
        "  for q_idx in query_indecies:\n",
        "    q, a = train_set[q_idx]\n",
        "    q = torch.flatten(q,-2,-1).squeeze(0)\n",
        "    queries.append(q)\n",
        "  query_indecies = random.sample(range(0, len(train_set)), num_pd)\n",
        "  for q_idx in query_indecies:\n",
        "    q, a = train_set2[q_idx]\n",
        "    q = torch.flatten(q,-2,-1).squeeze(0)\n",
        "    queries.append(q)\n",
        "  queries = torch.stack(queries)\n",
        "  return queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FfxLOABMo8KA"
      },
      "outputs": [],
      "source": [
        "#Synthetic queries definition\n",
        "def SynthQueries(num_queries):\n",
        "  queries = []\n",
        "  for q_idx in range(num_queries):\n",
        "    q = torch.clamp(synth_probs + torch.rand(784), min=0, max=1)\n",
        "    queries.append(q)\n",
        "  queries = torch.stack(queries)\n",
        "  return queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xNT1ib-aNdNX"
      },
      "outputs": [],
      "source": [
        "#Detection model definition\n",
        "class DetectionModel(nn.Module):\n",
        "  def __init__(self, in_dim, lstm_hidden_dim, mlp_hidden_dim, target_parameter_dim,out_dim):\n",
        "    super(DetectionModel, self).__init__()\n",
        "    # LSTM\n",
        "    self.lstm = nn.LSTM(in_dim, lstm_hidden_dim, 1, batch_first = True)\n",
        "    \n",
        "    # MLP\n",
        "    self.mlp1 = nn.Linear(lstm_hidden_dim,mlp_hidden_dim)\n",
        "    self.mlp2 = nn.Linear(target_parameter_dim,mlp_hidden_dim)\n",
        "    self.mlp3 = nn.Linear(mlp_hidden_dim*2,out_dim)\n",
        "    \n",
        "    # activation functions\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x, p): #num_queries\n",
        "\n",
        "    # compute encoding\n",
        "    o, _ = self.lstm(x)\n",
        "\n",
        "    # keep last layer\n",
        "    o = o[:,-1]\n",
        "\n",
        "    # pass through mlp\n",
        "    o = self.relu(self.dropout(self.mlp1(o)))\n",
        "    p = self.mlp2(p).unsqueeze(0)\n",
        "\n",
        "    o = torch.cat([o,p],-1)\n",
        "    o = self.mlp3(o)\n",
        "    return o\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhoIsABubryw",
        "outputId": "eeed8ce2-c96b-4733-98ce-4843113450c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21432])\n"
          ]
        }
      ],
      "source": [
        "ps = []\n",
        "for param in target_model.parameters():\n",
        "  ps.append(torch.flatten(param))\n",
        "ps = torch.cat(ps).to(device)\n",
        "print(ps.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "00sTXnXRfb4D"
      },
      "outputs": [],
      "source": [
        "detection_model_learning_rate = 0.00000000001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sGkvkOeUiGpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1759e5f5-b1f2-47a5-b569-a988f53b893c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H_tUHCfFNlsp"
      },
      "outputs": [],
      "source": [
        "#Create the detection model\n",
        "detection_model = DetectionModel(784,2048,1024,21432,2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "injYOXltt2Ra"
      },
      "outputs": [],
      "source": [
        "detection_model_optimizer = optim.Adam(detection_model.parameters(), lr=detection_model_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Esp4ydR8NnY3"
      },
      "outputs": [],
      "source": [
        "#Load the detection model if it exists\n",
        "if os.path.exists(\"detection_model.pt\"):\n",
        "    checkpoint = torch.load(\"detection_model.pt\")\n",
        "    detection_model.load_state_dict(checkpoint['state_dict'])\n",
        "    detection_model_optimizer.load_state_dict(checkpoint['optimizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCxRnmI1NpZr",
        "outputId": "c5a13c84-8966-423b-fa79-e70ca13f05c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3663349151611328\n",
            "1.4260262846946716\n",
            "1.3735675811767578\n",
            "1.3972859382629395\n",
            "1.3676543235778809\n",
            "1.3922427296638489\n",
            "1.3440635204315186\n",
            "1.3561939001083374\n",
            "1.3583173155784607\n",
            "1.3806659579277039\n",
            "1.3916305899620056\n",
            "1.3516534566879272\n",
            "1.3552793860435486\n",
            "1.369852602481842\n",
            "1.387883186340332\n"
          ]
        }
      ],
      "source": [
        "#Train the detection model to distinguish between adaptive retraining and benign queries\n",
        "for e in range(15):\n",
        "  total_loss = 0\n",
        "  #Train on benign\n",
        "  detection_model_optimizer.zero_grad()\n",
        "  Q = BenignQueries(1024).to(device).unsqueeze(0)\n",
        "  A = torch.tensor([0]).to(device)\n",
        "  O = detection_model(Q, ps)\n",
        "  loss = F.cross_entropy(O, A)\n",
        "  total_loss += loss.item()\n",
        "  loss.backward()\n",
        "  detection_model_optimizer.step()\n",
        "  \n",
        "  #Train on adaptive retraining\n",
        "  detection_model_optimizer.zero_grad()\n",
        "  Q = AdaptiveRetraining().to(device)\n",
        "  A = torch.tensor([1]).to(device)\n",
        "  O = detection_model(Q, ps)\n",
        "  loss = F.cross_entropy(O, A)\n",
        "  total_loss += loss.item()\n",
        "  loss.backward()\n",
        "  detection_model_optimizer.step()\n",
        "  \n",
        "  print(total_loss)\n",
        "  checkpoint = {   \n",
        "                'state_dict': detection_model.state_dict(),\n",
        "                'optimizer': detection_model_optimizer.state_dict(),\n",
        "      }\n",
        "  torch.save(checkpoint, \"detection_model.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrUtF5ubvYdt",
        "outputId": "f9021a3f-a674-4c05-d7fd-1730c319e04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38.5\n",
            "57.8\n",
            "3.6999999999999997\n"
          ]
        }
      ],
      "source": [
        "#Evaluate the detection model to distinguish between adaptive retraining and benign queries\n",
        "total_accuracy = 0\n",
        "false_positves = 0\n",
        "false_negatives = 0\n",
        "num_tests = 1000\n",
        "for e in range(num_tests):\n",
        "\n",
        "  #Pick random query tpye\n",
        "  r = random.randint(0, 2)\n",
        "  Q,A = 0,0\n",
        "  if r == 0:\n",
        "    Q = BenignQueries(1024).to(device).unsqueeze(0)\n",
        "    A = torch.tensor([0]).to(device)\n",
        "  elif r == 1:\n",
        "    Q = AdaptiveRetraining().to(device)\n",
        "    A = torch.tensor([1]).to(device)\n",
        "  else:\n",
        "    Q = SynthQueries(1024).to(device).unsqueeze(0)\n",
        "    A = torch.tensor([1]).to(device)\n",
        "\n",
        "  #Compute the output\n",
        "  O = detection_model(Q, ps).squeeze(0)\n",
        "\n",
        "  #Record accuracy\n",
        "  r = 0\n",
        "  if O[1] > O[0]:\n",
        "    r = 1\n",
        "    if A == 0:\n",
        "      false_positves += 1\n",
        "  else:\n",
        "    if A == 1:\n",
        "      false_negatives += 1\n",
        "  if A == r:\n",
        "    total_accuracy += 1\n",
        "\n",
        "total_accuracy /= num_tests\n",
        "total_accuracy *= 100\n",
        "print(total_accuracy)\n",
        "\n",
        "false_negatives /= num_tests\n",
        "false_negatives *= 100\n",
        "print(false_negatives)\n",
        "\n",
        "false_positves /= num_tests\n",
        "false_positves *= 100\n",
        "print(false_positves)\n",
        "\n",
        " \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TWML_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}